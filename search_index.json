[["index.html", "Air-Health SWS R targets (Last edited on 2025-07-03) 1 Introduction", " Air-Health SWS R targets (Last edited on 2025-07-03) 1 Introduction This is a summary of the project. "],["health-impact-function.html", "2 Health impact function", " 2 Health impact function This is the place to start because this defines the function that relates exposure to outcomes. "],["epidemiological-study-designs-source-sample-and-study-populations.html", "2.1 Epidemiological study designs: source, sample and study populations", " 2.1 Epidemiological study designs: source, sample and study populations This defines the weight of the evidence and the validity of the information base. "],["case-definition.html", "2.2 Case definition", " 2.2 Case definition Deal with it here. "],["relative-risk-odds-ratio-and-hazard-ratio.html", "2.3 Relative risk, odds ratio and hazard ratio", " 2.3 Relative risk, odds ratio and hazard ratio Some useful epidemiology background. E.g. Epidemiologists often use relative measures of effect, such as relative risk or odds ratio. These calculations give a quantified measure of the risk of getting a disease or other health outcome given an exposure to a risk factor. This is shown below. First we can define a simple example where there are some cases of disease (D) who have been exposed to the risk factor (DE) and other cases who were not (DN). We may also know the number of people who were exposed but remained healthy (HE) and the number of healthy people who were not exposed (HN). The total exposed can be represented as NE and the total not exposed as NN. Diseased Healthy Row_totals Exposed DE HE NE Not exposed DN HN NN The relative risk is the risk of disease given exposure calculated as the ratio of risks: \\[\\begin{equation} RR = \\frac{\\frac{DE}{NE}}{\\frac{DN}{NN}} \\end{equation}\\] The odds ratio is the ratio of the odds: OR = (DE/HE) / (DN/HN) We will summarise the material from the appendix of Hanigan, I.C., Geromboux, C., Horsley, J., Phelan, S., Jegasothy, E., Heathcote, K. and Morgan, G.G. (2020). Environmental health indicators for selected environmental hazards in New South Wales. Human Health and Social Impacts (HHSI) Node and the NSW Department of Planning, Industry and Environment (DPIE). https://cloudstor.aarnet.edu.au/plus/s/BUkjIIsJC3VTqmO DOI:10.17605/OSF.IO/YJ98D "],["the-paf-population-attributable-fraction.html", "2.4 The PAF (population attributable fraction)", " 2.4 The PAF (population attributable fraction) More stuff "],["tmrel.html", "2.5 TMREL", " 2.5 TMREL And the related TMRED. "],["study-population-and-health-outcomes.html", "3 Study population and health outcomes", " 3 Study population and health outcomes This is the place to start "],["source-sample-and-study-population.html", "3.1 Source, sample and study population", " 3.1 Source, sample and study population Some useful epidemiology background "],["exposure-assessment.html", "4 Exposure assessment", " 4 Exposure assessment This is the place to start "],["spatial-modelling-and-dealing-with-coverage-issues-or-missingness.html", "4.1 Spatial modelling and dealing with coverage issues or missingness", " 4.1 Spatial modelling and dealing with coverage issues or missingness Some useful background "],["counterfactual.html", "4.2 Counterfactual", " 4.2 Counterfactual Some useful background "],["link-population-health-and-environment-data.html", "5 Link population, health and environment data ", " 5 Link population, health and environment data "],["spatial-and-temporal-issues.html", "5.1 Spatial and temporal issues", " 5.1 Spatial and temporal issues Some useful background "],["attributable-number.html", "6 Attributable number", " 6 Attributable number This is the place to start "],["life-tables.html", "6.1 Life tables", " 6.1 Life tables Some useful background "],["getting-started.html", "A Getting started", " A Getting started The Air-Health SWS R targets pipeline requires R &gt;= 4.0.0 and access to CARDAT’s Environment_General data storage folder on Cloudstor. To run the Air Health SWS for the first time: Download and unzip or clone the air-health-sws-r-targets repository from the Code dropdown button. Load the R project. Open the _targets.R script. Edit the global variables years and states to set the study coverage - note with the initial parameters in the pipeline, years is limited to 2010-2015. Set the download_data boolean and dir_cardat to the correct path (parent directory of Environment_General). Open the main..R script. (This is not integral to the targets pipeline but is a place to keep all the useful commands for visualisiing, running and exploring the pipeline outside of the pipeline itself.) Begin running the script line-by-line from the top. renv should automatically install and activate. Install the packages using renv::restore() or try the alternative custom installation function install_pkgs() (installs the latest version if library not already available). This step may take some time. If you have set download_data &lt;- FALSE in _targets.R, uncomment and run the lines at the top of the Run pipeline section to authenticate your cloudstoR package’s access to Cloudstor. You should not need to authenticate again unless your credentials have changed. Continue on to visualise and run the pipeline. See the results of the desired target with tar_read(target_name). _targets.R and the custom functions called by targets (stored in R/) can be modified and extended to control pipeline output. "],["code-overview.html", "B Code overview", " B Code overview The structure and syntax of an R targets pipeline may be unfamiliar to you depending on your level of coding experience. Depending on your intended usage, some or all of the following may guide your understanding of the workflow. Links to further useful examples and documentation are provided. "],["r-targets-package.html", "B.1 R targets package", " B.1 R targets package The R targets package, a set of pipeline implementation and management tools, forms the basis of the Air health Scientific Workflow System. Using targets aids the reproducibility of analyses, tracking input data, parameters, code and dependencies to determine which steps need to be rerun when a change is detected. A targets pipeline is structured as a list of targets, each of which has a name and associated code block. The result of each target is saved and can be used in other targets by referring to it by name. On running the pipeline, each target is checked for changes to format, metadata and data, and is rerun if a change is detected. If there has been no change, the target is skipped (code is not run). Note that if a target changes, all dependent downstream targets will be run as the dependencies are recorded in target metadata. More complex pipelines can be set up with targets branching functionality and tarchetypes package. See the targets manual and documentation for further information. B.1.1 Function-oriented programming Targets is designed to be function-oriented, writing and calling functions. This is in contrast to the style of programming that runs step-by-step, top to bottom. An example of the latter: x &lt;- 2 y &lt;- 3 z &lt;- x*y In targets: do_multiply &lt;- function(a, b){ a*b } list( tar_target(x, 2), tar_target(y, 3), tar_target(z, do_multiply(x, y)) ) While for this example, there seems to be little benefit from creating a function (indeed one could simply write tar_target(z, x*y)), it aids code clarity and efficiency for more complex workflows. Clearly named functions can self-describe their intended purpose, with defined inputs (arguments) and output(s) (return value). Carefully defined generalised functions may be reused as needed within or across projects. "],["directory-and-file-structure.html", "B.2 Directory and File Structure", " B.2 Directory and File Structure The key files and folders of the Air Health SWS targets pipeline are as follows: ├── main.R ├── _targets.R ├── renv.lock ├── R/ ├──── func_analysis/ ├──── func_data/ ├──── func_helpers/ ├──── func_viz/ ├──── import_data/ ├──── pipelines/ ├── renv/ The main.R script is where you should start. It contains a few lines of code for restoring the packages, visualising the targets pipeline, running the pipeline and viewing target results. It does not make any changes to the workflow. Further exploratory analysis, outside of the pipeline, can be added here. The file renv.lock and folder renv/ are part of renv, a package management library. Opening the R project should automatically prompt renv to install and activate the project environment. Running renv::restore() will install required packages (as according to renv.lock). The _targets.R script forms the essential core of the targets pipeline, This is where the targets are defined, along with sourcing of required functions and specification of required libraries. See Section B.3 for more information. All custom functions are stored in the R/ folder. These are sourced near the top of the _targets.R script (and in main.R where needed). For clarity, the functions are arranged in a series of folders: func_analysis/: functions to analyse input data, acting on tidied and combined data func_data/: functions to combine and derive tidied input data to prepare for analysis func_helpers/: miscellaneous helper functions that are not directly concerned with the processing or analysis of data func_viz/: functions to visualise or output data import_data/: functions to create targets for the import and tidying of specific datasets pipelines/: examples of metaprogramming in targets package (not implemented) "],["targets.r-script.html", "B.3 _targets.R script", " B.3 _targets.R script As is standard with any _targets.R script, the structure is as follows: Load targets library (and tarchetypes if using) Source custom functions Set global variables Set target options (set what R packages are to be available to the pipeline) Define targets Custom functions used in the pipeline are loaded from the R/ folder. Though one can define the functions within _targets.R, keeping functions separately helps to keep code tidy. Additionally, the function files can be sourced or copied as needed for other projects. Global variables, here state and year, are a tidier way to specify parameters that are repeatedly used. In this case, the state and/or year are passed into various import data functions to indicate which subset of available data to use. The boolean download_data, to indicate whether to retrieve data from Cloudstor using the cloudstoR library, and data paths are also specified as global variables for ease of modification. Packages specified in tar_option_set are globally available to subsequent targets. If a target fails because a function cannot be found, double-check the required package is listed here. Finally, targets are defined in a list. For legibility, targets are divided into four lists, assigned to variables input, data, analysis and viz. These are then combined in the final list that defines the pipeline. Each of the target divisions has a corresponding function folder. input ....... R/import_data data ........ R/func_data analysis .... R/func_analysis viz ......... R/func_viz It is, of course, not necessary to use this organisation of targets and function files. All targets can be directly defined within the pipeline list and all function files can be stored under R/, or some alternative arrangement is possible. B.3.1 input targets Each element in the input list is the result of calling a function to import and tidy a specific dataset. Unlike the subsequent target lists data, analysis, and viz where an element defines one target, the functions called in input generate a series of targets. The series of targets tracks the data file(s), reads and tidies the data. Since each data has its own formats and quirks, the tidying process is unique to each and thus the code block is defined in the command argument of the target rather than as a separate function. (See any one of the functions under R/import_data.) Note that some of the import functions use tar_target_raw to include outside parameters such as the state or year. Datasets are not combined or dependent on one another at this stage. Data is manipulated into a tidy format (e.g. standardised names, converting column data types, converting to long format), and may be subsetted to relevant columns if the dataset is large. Further processing of data, such as calculation, combination and extraction takes place in data list. B.3.2 data targets The separate tidied datasets are combined and processed to a stage where the data is ready to be passed to an analysis function. In this pipeline, this covers the extraction of environmental exposure (from source rasters), calculation of counterfactual exposure, calculation of mortality rate in the impact population, application of mortality rate to the study population and aggregation of population-weighted exposure to study population regions. The end result of this section of targets is a data table of the study population with expected deaths (from application of mortality data) and exposure in baseline and counterfactual cases (from exposure rasters and defined counterfactual value). It is not necessary to gather all data into a single table, but avoids redundancy if you have several analysis targets working on the same data - you will not have to combine the data at the start of every analysis target. As a sidenote, it may be observed that some datasets are split over multiple files, for instance, the ABS meshblock files are by state. Depending on the dataset, tidying and processing may be best done file by file. This is possible through dynamic branching, where target B iterates over the output of other target(s) as defined by the pattern argument. Such dynamically-branched targets appear as a square with tar_glimpse or tar_visnetwork. (Read more at the targets manual) Consider the raster extraction to polygon step, one of the more time-consuming steps, where the inputs are exposure rasters (one file = one year) and ABS meshblock polygons (one file = one state). All the exposure rasters share the same projection, extent and resolution and thus can be stacked into a multi-layer raster - extracting a multi-layer raster does not cost much more than extracting from a single-layer raster. On the other hand, the cost of extraction is heavily dependent on the number and complexity of polygons. If an extra state were to be added to the pipeline, it would minimise cost to process and extract each set of state meshblocks separately via dynamic branching. Consequently, dynamic branching is used only on the ABS meshblocks dataset and not the exposure rasters, and branches recombined only after the extraction process is complete. B.3.3 analysis targets After data processing, analysis is performed through two targets - one defining the health impact response and the other applying the response to the processed data to calculate the attributable number of deaths. Further targets can be added to determine other health measures such as years of life lost. B.3.4 viz targets Naturally the results of analysis should be visualised in some form, at the very least for common-sense check. A target can be defined to produce a plot, or, if using tarchetypes, render an R Markdown report which can draw on output of other targets in its content. In the targets viz_an and leaflet_an, the code block no longer consists of a single function call. Instead, it is a series of data manipulation statements, aggregating and merging with spatial data, before calling the custom plot function. This structure keeps the plot function generalisable rather than specific to the data inputs. The data manipulation code can also be split off into a separation function or even a separate target if the results will be used in multiple targets. The R Markdown report is rendered in a target by specifying the appropriate .Rmd file in the tarchetypes::tar_render function. To minimise computation time and take advantage of the targets pipeline features, minimal processing should occur in R Markdown code. Target outputs are retrieved with tar_read or tar_load, Read more in the R targets manual - Literate Programming. B.3.5 Pipeline list The _targets.R script must end with a list of targets defining the pipeline, hence all targets previously defined are included as nested lists in this final list. One more target is introduced here to ensure the existence of the data path (if not downloading data) or valid authentication of Cloudstor access (if downloading data). It is set to run first with the argument priority = 1. "],["metaprogramming.html", "B.4 Metaprogramming", " B.4 Metaprogramming The R targets package includes metaprogramming tools (based on the rlang package), that is, tools using code to generate code. One of the simpler cases is static branching, generating a number of branched targets over a series of varying parameters, e.g. using different methods of analysis or modelling, or changing an input parameter to the modelling function. Full sets of targets (target factories) and full pipelines can also be generated on given parameters. These simplify the construction of a new targets pipeline, but is less customisable. See the stantargets package for an example of a target factory and [targets-shiny] for a fully-generated _targets.R script with Shiny interface built on top. An example of the pipeline-generating function resides in R/pipelines/write_pipeline_pm25_hia.R. Here the global variables in the original targets.R file are instead taken as arguments to the function. The targets pipeline code is written into the tar_helper function (which writes out to the _targets.R or other specified location). Variables (like states and years) are substituted into this code using the !! operator which forces the evaluation of the following expression. (There is also a !!! operator which both evaluates and unpacks, but it is not demonstrated here.) "],["case-study---who-guidelines.html", "C Case Study - WHO guidelines", " C Case Study - WHO guidelines The provided pipeline uses a relative risk of 1.06 per 10 µg/m3, with lower and upper confidence intervals (lci, uci) 1.02 and 1.08 respectively. The counterfactual is set as the minimum PM2.5 by state. Updated WHO 2021 recommendations specify a target for annual PM2.5 of 5 µg/m3. Interim targets are set at 35, 25, 15 and 10 µg/m3. The review which contributed to the setting of these target PM2.5 levels used a hazard ratio of 1.08 per 10 µg/m3. To explore the counterfactual scenario as described by the WHO recommended target, the calculation of counterfactual scenario and the health impact function must be modified. Counterfactual scenario The counterfactual exposure is calculated in the R target combined_exposures, The function call to do_env_counterfactual can be replaced with a block of code that adds the counterfactual exposure and delta to the output of target data_env_exposure_pm25. However, the do_env_counterfactual already includes functionality to add these columns based on an absolute value rather than using the minimum exposure by state. To use a counterfactual scenario based on the WHO’s recommended target of 5 µg/m3, alter the cf_mode argument from \"min\" to \"abs\", indicating the counterfactual is an absolute value. Provide a third argument cf_value as a numeric set to the WHO recommendation, 5. Your code will look similar to the following, in place of the combined_exposures target: ... # Provide counterfactual scenario and calculate delta tar_target( combined_exposures, do_env_counterfactual(data_env_exposure_pm25, &quot;abs&quot;, 5), pattern = map(data_env_exposure_pm25), ), ... Health impact response Next, modify the R target health_impact_function which returns a function to calculate the attributable number. In the do_health_impact_function, the argument exposure_response_func takes a three element numeric vector, representing the relative risk, lower confidence interval and upper confidence interval (in that order). Change the relative risk to that used by WHO, 1.08. If no confidence intervals are available, they may be set to NA. Your code should look similar to the following: ... tar_target(health_impact_function, do_health_impact_function( case_definition = &#39;crd&#39;, exposure_response_func = c(1.08, NA, NA), theoretical_minimum_risk = 0 ) ), ... "],["case-study---alternate-pm2.5-surfaces.html", "D Case Study - Alternate PM2.5 surfaces", " D Case Study - Alternate PM2.5 surfaces The PM2.5 exposure surfaces used in the provided pipeline comes from Surface PM2.5 V4.GL.02 dataset developed by van Donkelaar, A. et al. (2016). Alternative models of PM2.5 differing in method, temporal or spatial coverage and/or resolution which are more suited to your particular study may be used as the input exposure rasters. To use different PM2.5 surfaces, replace the call to function import_globalgwr_pm25_2010_2015 in _targets.R with targets importing and tidying the new raster data. A satellite-derived land use regression (SatLUR) modelled PM2.5 surface produced by Luke Knibbs is available on request. The preparation of van Donkelaar’s PM2.5 surfaces is performed in targets infile_globalgwr_pm25_2010_2015_files, infile_globalgwr_pm25_2010_2015 and tidy_env_exposure_pm25. (Only the latter two are present if downloading the data.) The first two targets are automatically generated by the tarchetypes::tar_files_input then tidied and gathered into a raster brick in tidy_env_exposure_pm25. This is performed in the following few lines: inputs &lt;- list( ..., # exposure rasters exposure = import_globalgwr_pm25_2010_2015(years, download = download_data, datadir_envgen = file.path(dir_cardat, dir_envgen)), ... ) Substitute in a tar_files_input pointing to the new input exposure raster files. Read and tidy the data in a tar_target, producing a RasterBrick (or stack) with named layers by year. Your code should look similar to the following. inputs &lt;- list( ..., # exposure rasters exposure = list( tar_files_input( infile_pm25, file.path( sprintf(&quot;YOUR_DATA_DIR/SatPM25_2000_2015/data_derived_rasters/satlur_pm25_ug_m3_%s.tif&quot;, years) ) ), tar_target( tidy_env_exposure_pm25, { b &lt;- brick(stack(infile_pm25)) # rename the layers of the RasterBrick with the year names(b) &lt;- years return(b) } ) ), ... ) Add additional processing steps in tidy_env_exposure_pm25 as needed to tidy the raster input. "],["case-study---alternate-pollutant.html", "E Case Study - Alternate Pollutant", " E Case Study - Alternate Pollutant The pipeline can be modified and extended to process and assess the impact of alternate pollutants or mixed-pollutant effects. You will need a set of suitable exposure rasters of your pollutant(s) of interest. A satellite-derived land use regression (SatLUR) modelled NO2 surface produced by Luke Knibbs is available on request. Exposure input New sets of pollutant rasters should be read and tidied in targets, similar to the code described in Case Study - Alternate PM2.5 surfaces. Give the targets unique appropriate names. For each pollutant, the tidied RasterBrick/RasterStack should be passed to the do_env_exposure to extract mean exposure at the meshblock level. The calculation of the counterfactual scenario follows (target combined_exposures in the initial pipeline) - either use the do_env_counterfactual function, or write your own custom code to calculate or read in a counterfactual. To ensure the pipeline does not rerun code needlessly, keep the processing of each pollutant in separate targets unless it is necessary or more efficient to process multiple pollutants at once. Consider a pipeline analysing both PM2.5 and NO2: if the source of NO2 data changes, only the NO2 exposure need be extracted, thus this target should be separate from the PM2.5 extraction step. Health Impact You will most likely need to alter the do_health_impact_function or develop your own function to calculate attributable number (or other measure of health impact). Elements to consider include: Health impact of interest Relative risk and what unit change it is based on Theoretical minimum risk (if applicable) Multi-pollutant effects "]]
